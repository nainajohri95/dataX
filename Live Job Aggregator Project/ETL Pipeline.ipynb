{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45c5a4d6-13b9-47a1-9af8-95a5593165d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Layer: Ingesting Data to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa1dc15-0bfc-4365-b207-4797c6ab3ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests,json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#defining the spark session\n",
    "spark = SparkSession.builder.appName(\"JobETL_Bronze\").getOrCreate()\n",
    "\n",
    "# defining the API key\n",
    "API_KEY = \"278025380e0c67b179f77be267daf50fa06806f4911fc87dcfc3af7aa0dc79a3\"\n",
    "\n",
    "roles = [\"Data Engineer\", \"Python Developer\", \"ETL Developer\", \"Spark Engineer\", \"Data Analyst\"]\n",
    "location = \"India\"\n",
    "all_jobs = []\n",
    "\n",
    "#Running a Loop over all the Job Roles\n",
    "for role in roles:\n",
    "    params={\n",
    "        \"engine\":\"google_jobs\",\n",
    "        \"q\":role, #\"q\": role → \"q\" yaha par role ka naam jayega (e.g., \"Data Engineer\")\n",
    "        \"location\":location,\n",
    "        \"api_key\":API_KEY\n",
    "    }\n",
    "    res = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "    jobs = res.json().get(\"jobs_results\", [])\n",
    "\n",
    "    # Print the whole response to see what's inside\n",
    "    # print(json.dumps(res.json(), indent=2))   \n",
    "     \n",
    "    # print(jobs)   getting API response\n",
    "\n",
    "    for job in jobs:\n",
    "        job[\"search_role\"] = role\n",
    "        all_jobs.extend(jobs)\n",
    "\n",
    "# bronze_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(job) for job in all_jobs]))\n",
    "# bronze_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(\"/mnt/lakehouse/bronze/jobs_raw\")\n",
    "\n",
    "# ✅ Convert Python list of dicts directly into Spark DF\n",
    "bronze_df = spark.createDataFrame(all_jobs)\n",
    "\n",
    "# ✅ Save to Delta (use managed table instead of DBFS path in free version)\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_jobs_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4725d17c-f254-45a0-ade0-6b6e8f81ed0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73fab389-52bd-4c9a-803c-afc5af7b0de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer: Cleaning and Structuring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6385ea04-9db2-4968-b1bf-b3bcf3a40747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JobETL_Silver\").getOrCreate()\n",
    "\n",
    "#reading bronze layer\n",
    "df = spark.read.table(\"bronze_jobs_raw\")\n",
    "\n",
    "#cleaning and structuring the data\n",
    "silver_df =  df.selectExpr(\n",
    "    \"title\",\n",
    "    \"company_name\",\n",
    "    \"location\",\n",
    "    \"description\",\n",
    "    \"job_id\",\n",
    "    \"detected_extensions.posted_at as posted_at\",\n",
    "    \"search_role\"\n",
    ").dropna(subset=[\"title\", \"company_name\", \"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef0389a-aea0-4fbd-a846-7329666fa752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c51cf0-c7af-43bf-8793-e2c899f7bc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = silver_df.withColumn(\"company_name\", trim(upper(col(\"company_name\"))))\n",
    "silver_df = silver_df.dropna(subset=[\"posted_at\"])\n",
    "silver_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"silver_jobs\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46500dc5-ec14-47fb-8148-b3297e964311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Golend Layer: Generate KPI Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcb98061-d19a-4d47-9b99-c0772efede25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JobETL_Gold\").getOrCreate()\n",
    "\n",
    "#reading silver layer\n",
    "df = spark.read.table(\"silver_jobs\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
