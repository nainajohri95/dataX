{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fce278c-58fe-4436-b811-d5ef077d0eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**STEPS TO FOLLOW**\n",
    "\n",
    "- Pick the Active Eligible table from config Table where active_flag=1, load_flag=1 and bq_to_gcs_status is 'COMPLETED'\n",
    "- Reads the parqut file from GCS (GCS path is like gcs_path/dt=/*.parquet) means take all the files present inside gcs path for any particular timestamp \n",
    "- Uses the latest dt(datetime file) folder\n",
    "- Writes the delta bronze path\n",
    "- Create the Table from bronze path to bronze dataset \n",
    "- Update the config table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30462a5a-82e2-442e-8f49-711dccb84c57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create widgets for Parametrization"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"mysql_host\",\"34.56.24.205\")\n",
    "mysql_host = dbutils.widgets.get(\"mysql_host\")\n",
    "\n",
    "dbutils.widgets.text(\"mysql_port\",\"3306\")\n",
    "mysql_port = dbutils.widgets.get(\"mysql_port\")\n",
    "\n",
    "dbutils.widgets.text(\"mysql_root\",\"root\")\n",
    "mysql_user = dbutils.widgets.get(\"mysql_root\")\n",
    "\n",
    "dbutils.widgets.text(\"mysql_db\",\"GCPMigrationMeta\")\n",
    "mysql_db = dbutils.widgets.get(\"mysql_db\")\n",
    "\n",
    "dbutils.widgets.text(\"mysql_password\",\"Anupamnaina#18\")\n",
    "mysql_password = dbutils.widgets.get(\"mysql_password\")\n",
    "\n",
    "dbutils.widgets.text(\"bq_sa_key\",'/Volumes/workspace/default/csv/datamigrationproject-483310-739969975183.json')\n",
    "GOOGLE_APPLICATION_CREDENTIALS = dbutils.widgets.get(\"bq_sa_key\")\n",
    "\n",
    "# Unity Catalog widgets\n",
    "dbutils.widgets.text(\"catalog_name\", \"bronze\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "\n",
    "dbutils.widgets.text(\"schema_name\", \"raw\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d56720a-c268-4352-890d-4d37b260cc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mysql-connector-python google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c67289-1386-4eff-a617-4d0d42060cd9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing Library for connection"
    }
   },
   "outputs": [],
   "source": [
    "import json, os, datetime as dt\n",
    "import mysql.connector as mc\n",
    "from google.cloud import bigquery\n",
    "from contextlib import contextmanager \n",
    "# contextlib python library help us to setup and clean up the resources automatically\n",
    "\n",
    "MYSQL = {\n",
    "    \"host\": dbutils.widgets.get(\"mysql_host\").strip(),\n",
    "    \"port\": int(dbutils.widgets.get(\"mysql_port\")),\n",
    "    \"db\":   dbutils.widgets.get(\"mysql_db\").strip(),\n",
    "    \"user\": dbutils.widgets.get(\"mysql_user\").strip(),\n",
    "    \"pwd\":  dbutils.widgets.get(\"mysql_password\"),\n",
    "}\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "schema_name  = dbutils.widgets.get(\"schema_name\").strip()\n",
    "\n",
    "# GCP service account key path (convert dbfs:/ to /dbfs/)   \n",
    "GCP_KEY = dbutils.widgets.get(\"GOOGLE_APPLICATION_CREDENTIALS\").replace('dbfs:/','/dbfs/') \n",
    "\n",
    "assert os.path.exists(GCP_KEY), f\"GCP key not found at {GCP_KEY}\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY  # auth for GCS SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c6f07ad-2700-48dc-a376-41069f15cb80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Settingup MYSQL connection"
    }
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def mysql_conn():\n",
    "    conn = mc.connect(\n",
    "        host = MYSQL['host'], \n",
    "        port = MYSQL['port'],\n",
    "        user = MYSQL['user'],\n",
    "        password = MYSQL['pwd'],\n",
    "        database = MYSQL['db']\n",
    "    )\n",
    "    try:\n",
    "        yield conn\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to MYSQL \\nHere is the error -: \\n{e}\")\n",
    "    # yield returns a value temporarily and pauses the function, then resumes later from the same place.\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110f6aa2-c3e1-426d-86a4-299da7d53838",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetch the eligible table"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_eligible_rows():\n",
    "    \"\"\"\n",
    "    Fetch rows from config_table that are eligible for processing:\n",
    "    - active_flag=1\n",
    "    - load_flag=1\n",
    "    - bq_to_gcs_status='COMPLETED'\n",
    "    - gcs_to_bronze_status in ('NOT_STARTED','FAILED')\n",
    "    \"\"\"\n",
    "    with mysql_conn() as conn:\n",
    "        cur = conn.cursor(dictionary=True)\n",
    "        cur.execute(\"\"\"\n",
    "          SELECT table_name, gcs_path, target_path\n",
    "          FROM config_table\n",
    "          WHERE active_flag=1\n",
    "            AND load_flag=1\n",
    "            AND bq_to_gcs_status='COMPLETED'\n",
    "            AND gcs_to_bronze_status IN ('NOT_STARTED','FAILED')\n",
    "          ORDER BY table_name\n",
    "        \"\"\")\n",
    "        rows = cur.fetchall()\n",
    "        cur.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6028ba2-284f-42f1-89c7-c229138f84a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Bronze status"
    }
   },
   "outputs": [],
   "source": [
    "def set_bronze_status(table_name, status, err = None):\n",
    "     with mysql_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "        if status == 'IN_PROGRESS':\n",
    "            # Mark as in progress and clear any previous error \n",
    "            cur.execute(\"\"\"UPDATE GCPMigrationMeta.config_table set gcs_to_bronze_status=\"IN_PROGRESS\", last_run_ts=NOW(), error_message=NULL where table=%s\"\"\", (table_name))\n",
    "\n",
    "        elif status == 'COMPLETED':\n",
    "            # Mark as in progress and clear any previous error \n",
    "            cur.execute(\"\"\"UPDATE GCPMigrationMeta.config_table set gcs_to_bronze_status=\"COMPLETED\", last_run_ts=NOW(), error_message=NULL where table=%s\"\"\", (table_name))\n",
    "\n",
    "        else:\n",
    "            # Mark as in progress and clear any previous error \n",
    "            cur.execute(\"\"\"UPDATE GCPMigrationMeta.config_table set gcs_to_bronze_status=\"FAILED\", last_run_ts=NOW(), error_message = '{str(err)[:2000] if err else \"FAILED\"},                     WHERE table_name = '{table_name}')    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44d4363-5d9c-48a0-8b6f-18c366c74066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "Reset the load_flag"
    }
   },
   "outputs": [],
   "source": [
    "def reset_load_flag(table_name):\n",
    "    with mysql_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"\"\"UPDATE CPMigrationMeta.config_table set load_flag=0 where table_name=%s\"\"\", (table_name))\n",
    "        conn.commit()\n",
    "        cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba2efdef-d89a-4b77-bad9-0c79a1ff65e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating GCS Client"
    }
   },
   "outputs": [],
   "source": [
    "# This will return us the bucket name\n",
    "def gcs_client():\n",
    "    # storage is a gcs inbuilt fun that will give us the storage details\n",
    "    return storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a4329b-347d-4d91-b937-ab83b83d9f76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split the CGS Bucket URI"
    }
   },
   "outputs": [],
   "source": [
    "def split_gcs(gcs_uri:str):\n",
    "    \"\"\"Split a GCS URI into bucket and path.\"\"\"\"\n",
    "    assert gcs_uri.startwith(\"gs://\")\n",
    "    p = urlparse(gcs_uri)\n",
    "    return p.netloc, p.path.lstrip(\"/\")  #netloc will give us the bucket name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ae916b-3357-4466-8a32-30d4c6e4c037",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Getting the latest datetime uri"
    }
   },
   "outputs": [],
   "source": [
    "# We have to get the uri for the lastest one \n",
    "def latest_dt_uri(gcs_base: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the latest dt=YYYYMMDDTHHMMSSZ folder under gcs_base.\n",
    "    Return a wildcard URI for all Parquet files in that folder.\n",
    "    \"\"\"\n",
    "    bucket_name, base_prefix = split_gs(gcs_base.rstrip(\"/\"))\n",
    "    cli = gcs_client()\n",
    "\n",
    "    # List all objects under base_prefix/dt=\n",
    "    search_prefix = f\"{base_prefix}/dt=\"\n",
    "    dts = set()\n",
    "    for blob in cli.list_blobs(bucket_name, prefix=search_prefix):\n",
    "        # Look for '.../dt=xxxxx/' in blob.name or files under it\n",
    "        m = re.search(r\"dt=([\\dT]+Z)/\", blob.name)\n",
    "        if m:\n",
    "            dts.add(m.group(1))\n",
    "    if not dts:\n",
    "        # fallback: allow wildcard if no dt folders found\n",
    "        return f\"gs://{bucket_name}/{base_prefix}/dt=*/*.parquet\"\n",
    "    latest = sorted(dts)[-1]\n",
    "    return f\"gs://{bucket_name}/{base_prefix}/dt={latest}/*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8909e00e-3d77-409f-b839-db44b823abea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# how this latest_dt_uri fun will work\n",
    "latest_dt_url(\"bigquerytogcsmigration/exports/ods/ods_order_items\")\n",
    "now from this folder this function will return the latest datetime uri5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f4b5bd-0ae3-4d65-b507-3648bce3c993",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check existence of directory"
    }
   },
   "outputs": [],
   "source": [
    "# We have to check that the given directory path like this \"bigquerytogcsmigration/exports/ods/ods_order_items\" exists or not\n",
    "def ensure_dir(local_path: str):\n",
    "    \"\"\"Create a local directory if does not exist\"\"\"\n",
    "    os.markedirs(local_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0df6be3f-82ef-4955-8f74-1b7ff7481552",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function to get the wildcrad understanding"
    }
   },
   "outputs": [],
   "source": [
    "def prefix_from_wildcard(gcs_uri: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Given a GCS URI with wildcard, return (bucket, prefix) for listing.\n",
    "    Example: gs://bucket/foo/bar/dt=.../*.parquet -> (bucket, 'foo/bar/dt=.../')\n",
    "    \"\"\"\n",
    "    bucket, path = _split_gs(gcs_uri)\n",
    "    if \"*\" in path:\n",
    "        prefix = path[:path.rfind(\"/\") + 1]\n",
    "    else:\n",
    "        prefix = path if path.endswith(\"/\") else path + \"/\"\n",
    "    return bucket, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ccacd7-74c2-47b5-8c1d-29dd4ca5f895",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GCS toDBFS copy of prefix"
    }
   },
   "outputs": [],
   "source": [
    "def copy_gcs_prefix_to_dbfs(gcs_uri_wildcard: str, dbfs_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Copy all objects under the wildcard's parent prefix from GCS to DBFS directory.\n",
    "    Returns the DBFS directory containing the downloaded Parquet files.\n",
    "    \"\"\"\n",
    "    bucket, prefix = _prefix_from_wildcard(gcs_uri_wildcard)\n",
    "    cli = gcs_client()\n",
    "    bkt = cli.bucket(bucket)\n",
    "\n",
    "    local_dir = dbfs_dir.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    if os.path.exists(local_dir):\n",
    "        shutil.rmtree(local_dir)\n",
    "    ensure_dir(local_dir)\n",
    "\n",
    "    n = 0\n",
    "    for blob in cli.list_blobs(bucket, prefix=prefix):\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "        local_file = os.path.join(local_dir, os.path.basename(blob.name))\n",
    "        blob.download_to_filename(local_file)\n",
    "        n += 1\n",
    "    if n == 0:\n",
    "        raise FileNotFoundError(f\"No objects found under gs://{bucket}/{prefix}\")\n",
    "    print(f\"↳ Copied {n} file(s) from gs://{bucket}/{prefix} → {dbfs_dir}\")\n",
    "    return dbfs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6457a6e-1422-44ad-83c7-b88ba06a833b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert path toDBFS format if not already in DBFS"
    }
   },
   "outputs": [],
   "source": [
    "def to_dbfs(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a path to DBFS format if not already in DBFS.\n",
    "    \"\"\"\n",
    "    p = path.strip()\n",
    "    if p.startswith(\"dbfs:/\") or p.startswith(\"dbfs:\"):\n",
    "        return p\n",
    "    if p.startswith(\"/\"):\n",
    "        return \"dbfs:\" + p\n",
    "    return \"dbfs:/\" + p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706f5035-51e0-42bf-ae7f-6bebff921ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main ETL Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ebedc8d-ea4e-4e02-a486-f6398e60ddae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rows = fetch_eligible_tables()\n",
    "if not rows:\n",
    "    print(\"Nothing to process (No eilgible tables)\")\n",
    "else:\n",
    "    spark.sql(f\"Create database if not exist {bronze}\")\n",
    "    print(f\"Processing {len(rows)} table(s): {[r['table_name'] for r in rows]}\")\n",
    "\n",
    "    for r in rows:\n",
    "        t = r['table_name']\n",
    "        gcs_base = r['gcs_path']\n",
    "        dst_path = to_dbfs(r['target_path'])\n",
    "        print(f\"Processing Started for ----> {t}...\")\n",
    "\n",
    "    try:\n",
    "        set_bronze_status(t, 'In_Progress')\n",
    "\n",
    "        # Pick the latest dt folder in GCS of table\n",
    "        src_uri = latest_dt_uri(gcs_base)\n",
    "        print(f\"latest_uri: {src_uri}\")   \n",
    "\n",
    "        # Copy the parquet files from GCS to DBFS Directory\n",
    "        stage_dir = f\"dbfs:/tmp/gcs_stage/{t}\"\n",
    "        stage_dir = copy_gcs_prefix_to_dbfs(src_uri, stage_dir)\n",
    "        print(f\"\\n{t}: staged to {stage_dir}\")\n",
    "\n",
    "        df = spark.read.parquet(stage_dir)\n",
    "\n",
    "        # Write it to the Delta Table format\n",
    "        # main usecase is to bring all the tables to the databrics azure platform present in gcs bucket\n",
    "        table_name = t\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "        df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(full_table_name)\n",
    "\n",
    "        # Register as delta table to databricks using destination path\n",
    "        spark.sql(f\"Drop Table if Exists {catalog_name}.{schema_name}.{t}\")\n",
    "        spark.sql(f\"Create table {catalog_name}.{schema_name}.{t} using delta location '{dst_path}'\")\n",
    "        print(f\"\\n{t}: written to {dst_path}\")\n",
    "        reset_load_flag(t)\n",
    "        set_bronze_status(t, \"Completed\")\n",
    "        print(f\"{t} Table has been written at {dst_path}\")\n",
    "\n",
    "    except:\n",
    "        set_bronze_status(t,'Failed')\n",
    "        print(f\"{t} Table has failed to write at {dst_path}\")\n",
    "\n",
    "dbutils.fs.rm(\"dbfs:/tmp/gcs_stage\",recu)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5231678388260692,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GCS_TO_DBX",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
